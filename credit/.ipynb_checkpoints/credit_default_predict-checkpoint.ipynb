{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: \n",
    "Here I explore the UCI Default of Credit Cards data set (available on Kaggle:https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset  ) and compare several machine learning models (with/without feature engineering) for predicting default.\n",
    "\n",
    "This notebook proceeds as follows:\n",
    "1. Read and Explore the Data\n",
    "    1. Look for NAs, outliers, any unusual distributions\n",
    "    1. Look at associations/correlations\n",
    "1. Prepare the Data\n",
    "    1. Cleaning the Data\n",
    "    1. Feature Engineering\n",
    "        1. Indicator variables\n",
    "        1. Feature interactions\n",
    "\n",
    "Introduction\n",
    "This notebook was created to learn basic techniques of data manipulation and machine learning. The idea is to use the dataset UCI_Credit_Card to improve basic skills of data cleaning, data analysis, data visualization and machine learning. It is primarily intended to help myself understanding what to do and how. Any feedback is welcome.\n",
    "\n",
    "Variables\n",
    "There are 25 variables:\n",
    "\n",
    "* ID: ID of each client\n",
    "* LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n",
    "* SEX: Gender (1=male, 2=female)\n",
    "* EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
    "* MARRIAGE: Marital status (1=married, 2=single, 3=others)\n",
    "* AGE: Age in years\n",
    "* PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
    "* PAY_2: Repayment status in August, 2005 (scale same as above)\n",
    "* PAY_3: Repayment status in July, 2005 (scale same as above)\n",
    "* PAY_4: Repayment status in June, 2005 (scale same as above)\n",
    "* PAY_5: Repayment status in May, 2005 (scale same as above)\n",
    "* PAY_6: Repayment status in April, 2005 (scale same as above)\n",
    "* BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n",
    "* BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
    "* BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
    "* BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
    "* BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
    "* BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
    "* PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
    "* PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
    "* PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
    "* PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
    "* PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
    "* PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n",
    "* default.payment.next.month: Default payment (1=yes, 0=no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        data_path = os.path.join(dirname, filename)\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-85ccdf353800>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_o\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#df_o.describe()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_o\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#df.columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#df_o.info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_path' is not defined"
     ]
    }
   ],
   "source": [
    "df_o = pd.read_csv(data_path, nrows =10000)\n",
    "#df_o.describe()\n",
    "df_o.sample(10)\n",
    "#df.columns\n",
    "#df_o.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello \\nthere')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple fucntion for describing the data\n",
    "def quick_analysis(df):\n",
    "    print('\\n Rows and Columns:')\n",
    "    print(df.shape)\n",
    "    print('\\n Data Types:')\n",
    "    print(df.dtypes)\n",
    "\n",
    "    print('\\n Column Names:')\n",
    "    print(df.columns)\n",
    "    print('\\n Null Values:')\n",
    "    print(df.apply(lambda x: sum(x.isnull()) / len(df)))\n",
    "quick_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=df[df.dtypes[(df.dtypes==\"float64\")|(df.dtypes==\"int64\")]\n",
    "                        .index.values].hist(figsize=[14,14], bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these could use a transformation to make the data more normal. That sould help for logisitic regression, though likely not for decision tree based classifier models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NAs\n",
    "df = df_o.dropna(axis = 0, how ='any') \n",
    "\n",
    "print(\"Old data frame length:\", len(df_o), \"\\nNew data frame length:\",  \n",
    "       len(df), \"\\nNumber of rows with at least 1 NA value: \", \n",
    "       (len(df_o)-len(df))) \n",
    "\n",
    "# Set the ID column as the index\n",
    "df.set_index('ID',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {\"default.payment.next.month\":\"default_next\"},inplace=True)\n",
    "df.rename(str.lower, axis='columns',inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Associations\n",
    "\n",
    "We'll start with a heatmap of all the correlations between features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "corr = df.corr()\n",
    "#cmap = sn.diverging_palette(255, 133, l=60, n=7, center=\"dark\")\n",
    "ax = sn.heatmap(corr,cmap='RdBu_r',vmin=-1,vmax=1)\n",
    "\n",
    "ax.set_title(\"Correlation for all Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some interesting trends that emerge here. The PAY, BILL, and PAY AMNT   features each form correlation clusters with themselves (these are the 6x6 squares that are present). \n",
    "\n",
    "The BILL attributes are the most highly correlated with each other. We could perhaps reduce these down to one variable and perhaps engineer a few more interesting features. \n",
    "\n",
    "The PAY featurs, which you'll recall are represneted on a -2 to 6 scale (with higher (positive) values indicative of a longer duration of missed payments) are also correlated each other, though decreasingly so for the older features in the group.   \n",
    "\n",
    "It's also worth noting that the PAY features are the feature with the highest correlation with defaulting on the credit card (our target variable \"default_next\")--though its modest ~0.35 or so. Yet PAY_AMT and BILL_AMT are even less correlated with default. The demographic info (age, education, etc) are also poorly corrlation with our target variable.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(9,6))\n",
    "plt.subplots_adjust(wspace=.4)\n",
    "fig.suptitle('Demographic Trends with Gender')\n",
    "sn.catplot(ax=axes[0],x='sex',y='education',hue='default_next',kind='box',data=df) \n",
    "sn.catplot(ax=axes[1],x='sex',y='marriage',hue='default_next',kind='box',data=df) \n",
    "sn.catplot(ax=axes[2],x='sex',y='age',hue='default_next',kind='box',data=df) \n",
    "\n",
    "plt.close(2)\n",
    "plt.close(3)\n",
    "plt.close(4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next we take a closer look at how age relates with default, with tiers broken down based on education level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.catplot(x=\"education\", y=\"age\", hue=\"default_next\",\n",
    "            kind=\"violin\", split=True, data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Education levels 0, 4,5 and 6 deserve a closer look. 4 though six are catagarized as \"unknown\", but the distributions for 4,5,6 are striking (in the violin plot below) because the are each quite different from each other as fron level 1,2,3. Level 5 in particulare may form a good indicator variable for default. Where as 4 and 6 have no apparent risk for default. But we should first look at how many instance are in each of these groups (if it's very few, then they could be dropped altogether). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"education\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very few are in education levels 0,4,5, or 6. Level 0 we will drop. Level 5 and age greater than 50 may be an indicator of default based on the above plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.catplot(x=\"sex\", y=\"age\", hue=\"default_next\",\n",
    "            kind=\"violin\", split=True, data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the age vs sex violin plot we observe the slight trend of lower peak default age for both men and women alike. Nother terrible interesting here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we look at simiar age-based trends, but based on marriage status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.catplot(x=\"marriage\", y=\"age\", hue=\"default_next\",\n",
    "            kind=\"violin\", split=True, data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.catplot(x=\"marriage\", y=\"age\", hue=\"default_next\",\n",
    "            kind=\"box\", data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"marriage\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "attributes_pay = ['pay_0','pay_2','pay_3','pay_4','pay_5','pay_6','default_next']\n",
    "#scatter_matrix(df[attributes_pay],figsize=(10,7.5))\n",
    "#attributes_pay = ['PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']\n",
    "corr = df[attributes_pay].corr()\n",
    "ax = sn.heatmap(corr, cmap='RdBu_r',center=0)\n",
    "ax = ax.set_title('Correlation of PAY features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_bil = ['bill_amt1','bill_amt2','bill_amt3','bill_amt4','bill_amt5',\n",
    "                  'bill_amt6','default_next']\n",
    "#scatter_matrix(df[attributes_bil],figsize=(15,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the top row, it's not surprising to see that BILL_AMT1 is most correlated with BILL_AMT2 and progressively less with bills amount for previous montion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[attributes_bil].corr()\n",
    "ax=sn.heatmap(corr, cmap='RdBu_r',center=0)\n",
    "ax = ax.set_title('Correlation of BILL features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_payamt = ['pay_amt1','pay_amt2','pay_amt3','pay_amt4','pay_amt5',\n",
    "                     'pay_amt6','default_next']\n",
    "#scatter_matrix(df[attributes],figsize=(15,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[attributes_payamt].corr()\n",
    "_=sn.heatmap(corr, cmap='RdBu_r',center=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df.default_next\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "x=df.copy()\n",
    "x.drop(['default_next'],axis=1,inplace=True)\n",
    "x.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline models\n",
    "\n",
    "Here I'm giong to employ two common classifiers to get an idea of baseline predictive performance prior to any feature engineering or tuning. \n",
    "\n",
    "First up: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score,recall_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "def predict_logistic(x,y,features=[],do_scale=True,do_resamp=False, show_cm=False,normTF=True):\n",
    "    \n",
    "    if len(features)>0:\n",
    "            x=x[features]\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x,y,test_size=0.2,random_state=3)\n",
    "    if do_resamp:\n",
    "        sm = SMOTE(random_state=0, ratio = 1.0)\n",
    "        x_train, y_train = sm.fit_sample(x_train, y_train)\n",
    "    \n",
    "    model = LogisticRegression(random_state=0,solver='lbfgs', multi_class='ovr')\n",
    "    if do_scale:\n",
    "        classifier = Pipeline(steps = [('scaler',StandardScaler()),('model',model)])\n",
    "    else:\n",
    "        classifier = Pipeline(steps = [('model',model)])\n",
    "        \n",
    "    classifier.fit(x_train,y_train)\n",
    "\n",
    "    y_predict = classifier.predict(x_val)\n",
    "    auc = roc_auc_score(y_val, y_predict)\n",
    "    print('AUC (log-reg)=',round(auc,5))\n",
    "    recall = recall_score(y_val, y_predict)\n",
    "    print('Recall(log-reg)',round(recall,5))\n",
    "    \n",
    "    if show_cm:\n",
    "        plot_confusion_matrix(y_val, y_predict, classes=['no default','default'],\n",
    "                          normalize=normTF,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def predict_dtree(x,y,features=[],do_scale=True,do_resamp=False, show_cm=False,normTF=True):\n",
    "    \n",
    "    if len(features)>0:\n",
    "            x=x[features]\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x,y,test_size=0.2,random_state=3)\n",
    "    if do_resamp:\n",
    "        sm = SMOTE(random_state=0, ratio = 1.0)\n",
    "        x_train, y_train = sm.fit_sample(x_train, y_train)\n",
    "    \n",
    "    model = DecisionTreeClassifier(max_depth=10,random_state=3)\n",
    "    if do_scale:\n",
    "        classifier = Pipeline(steps = [('scaler',StandardScaler()),('model',model)])\n",
    "    else:\n",
    "        classifier = Pipeline(steps = [('model',model)])\n",
    "        \n",
    "    classifier.fit(x_train,y_train)\n",
    "\n",
    "    y_predict = classifier.predict(x_val)\n",
    "    auc = roc_auc_score(y_val, y_predict)\n",
    "    print('AUC (D-Tree)=',round(auc,5))\n",
    "    recall = recall_score(y_val, y_predict)\n",
    "    print('Recall (D-Tree)',round(recall,5))\n",
    "    \n",
    "    if show_cm:\n",
    "        plot_confusion_matrix(y_val, y_predict, classes=['no default','default'],\n",
    "                          normalize=normTF,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def predict_rand_for(x,y,features=[],do_scale=True,do_resamp=False, show_cm=False,normTF=True):\n",
    "    \n",
    "    if len(features)>0:\n",
    "            x=x[features]\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x,y,test_size=0.2,random_state=3)\n",
    "    if do_resamp:\n",
    "        sm = SMOTE(random_state=0, ratio = 1.0)\n",
    "        x_train, y_train = sm.fit_sample(x_train, y_train)\n",
    "    \n",
    "    model = RandomForestClassifier(random_state=3)\n",
    "    if do_scale:\n",
    "        classifier = Pipeline(steps = [('scaler',StandardScaler()),('model',model)])\n",
    "    else:\n",
    "        classifier = Pipeline(steps = [('model',model)])\n",
    "        \n",
    "    classifier.fit(x_train,y_train)\n",
    "\n",
    "    y_predict = classifier.predict(x_val)\n",
    "    auc = roc_auc_score(y_val, y_predict)\n",
    "    print('AUC (D-Tree)=',round(auc,5))\n",
    "    recall = recall_score(y_val, y_predict)\n",
    "    print('Recall (D-Tree)',round(recall,5))\n",
    "    \n",
    "    if show_cm:\n",
    "        plot_confusion_matrix(y_val, y_predict, classes=['no default','default'],\n",
    "                          normalize=normTF,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All original features\n",
    "predict_logistic(x,y,features=[], do_scale=True,do_resamp=True,show_cm=False)\n",
    "predict_dtree(x,y,features=[], do_scale=True,do_resamp=True,show_cm=False)\n",
    "predict_rand_for(x,y,features=[], do_scale=True,do_resamp=True,show_cm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are a basic set of features that appear to be somewhat correlated with default prediction\n",
    "features_sub1=['limit_bal','pay_amt1','education', 'marriage','pay_0']    \n",
    "predict_logistic(x,y,features=features_sub1, do_scale=True,do_resamp=True,show_cm=False)\n",
    "predict_dtree(x,y,features=features_sub1, do_scale=True,do_resamp=True,show_cm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we've added some 'bill_amt' features to see if the are at all usefull (not really)\n",
    "features_sub2=['limit_bal','pay_amt1','education', 'marriage','pay_0','bill_amt1','bill_amt2','bill_amt3']  \n",
    "predict_logistic(x,y,features=features_sub2, do_scale=True,do_resamp=True,show_cm=False)\n",
    "predict_dtree(x,y,features=features_sub2, do_scale=True,do_resamp=True,show_cm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much do additional pay features help? Ans: a small bit for the recall\n",
    "features_sub3=['limit_bal','pay_amt1','education', 'marriage','pay_0','pay_2','pay_3','pay_4']    \n",
    "predict_logistic(x,y,features=features_sub1, do_scale=True,do_resamp=True,show_cm=False)\n",
    "predict_dtree(x,y,features=features_sub3, do_scale=True,do_resamp=True,show_cm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "First lets explore bill amount. Rather than just looking at bill *amount*, lets add bill ratio, for the current and previous bills: BILL_1to2 = BILL_AMT1 / BILL_AMT2. A large increase in bill amount from one month to the next might correlate with default.  Similarly, pay amount might reaveal a similar trend: PAY_1to2 = PAY_AMT1 /PAY_AMT2. Another choice might be BILL_AMT1/PAY_AMT1. A higher bill-to-payment ratio seem likely to we a warning sign of default\n",
    "\n",
    "The Diffs and the Ratios capture somewhat overlapping trends. Diff maintains scaling which may be useful considering higher borrowing levels may be indicative of default. But the ratios which are be definition normalized, may be better for their simplicity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Diffs and the Ratios capture somewhat overlapping trends. Diff maining scaling, \n",
    "# Which may \n",
    "import scipy.stats as ss\n",
    "\n",
    "# One Month Ratios\n",
    "f1 = df[\"bill_amt1\"]/df[\"bill_amt2\"]\n",
    "f2 = df[\"pay_amt2\"]/df[\"pay_amt1\"]\n",
    "\n",
    "# Four Month Rations\n",
    "f3 = df[\"bill_amt1\"]/df[\"bill_amt5\"]\n",
    "f4 = df[\"pay_amt5\"]/df[\"pay_amt1\"]\n",
    "\n",
    "# One Month Diffs\n",
    "f5 = df[\"bill_amt1\"]-df[\"bill_amt2\"]\n",
    "f6 = df[\"pay_amt2\"]-df[\"pay_amt1\"]\n",
    "\n",
    "# Four Month Diffs\n",
    "f7 = df[\"bill_amt1\"]-df[\"bill_amt5\"]\n",
    "f8 = df[\"pay_amt5\"]-df[\"pay_amt1\"]\n",
    "\n",
    "f9 = df[\"bill_amt1\"]/df[\"pay_amt1\"]\n",
    "f10 = df[\"bill_amt1\"] -df[\"pay_amt1\"]\n",
    "\n",
    "atts =['bill_amt1','bill_amt2','bill_amt3','bill_amt4','bill_amt5','bill_amt6']\n",
    "f11 = df[atts].std(axis=1)\n",
    "#bils = ['bill_amt1','bill_amt2','bill_amt3','bill_amt4','bill_amt5','bill_amt6']\n",
    "#bil_std = ss.s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporate the new features into the dataframe\n",
    "df_f = df.copy()\n",
    "new_cols = ['F1','F2','F3','F4','F5','F6','F7','F8','F9','F10']\n",
    "df_f['f1'] = f1\n",
    "df_f['f2'] = f2\n",
    "df_f['f3'] = f3\n",
    "df_f['f4'] = f4\n",
    "df_f['f5'] = f5\n",
    "df_f['f6'] = f6\n",
    "df_f['f7'] = f7\n",
    "df_f['f8'] = f8\n",
    "df_f['f9'] = f9\n",
    "df_f['f10'] = f10\n",
    "df_f['f11'] = f11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_f.columns.tolist()\n",
    "#print(cols[23])\n",
    "cols_new=cols[0:23]\n",
    "cols_new = cols_new + cols[24:]\n",
    "cols_new.append(cols[23])\n",
    "print(cols_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = df_f[cols_new]\n",
    "\n",
    "corr = df_f.corr()\n",
    "#cmap = sn.diverging_palette(255, 133, l=60, n=7, center=\"dark\")\n",
    "fig,ax = plt.subplots(figsize=(13,8))\n",
    "_=sn.heatmap(corr,cmap='RdBu_r',vmin=-1,vmax=1)\n",
    "\n",
    "\n",
    "ax.set_title(\"Correlation for all Features (inc Engineered)\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We are dropping quite a few row here due to NAs generated by the feature vars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new features with ratio are problematic due to generation of infinity when denomenator is 0. Current version drop these features. Should be further explored. Note that I've dropped the colums (for now) rather than droping rows (would have needed to drop 1/4 of rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NAs\n",
    "df_fdrop = df_f.replace([np.inf, -np.inf], np.nan)\n",
    "df_fdrop = df_fdrop.dropna(axis=1, how=\"any\")\n",
    "#df_fdrop = df_f.dropna(axis = 0, how ='any') \n",
    "\n",
    "print(\"Old data frame length:\", len(df_f), \"\\nNew data frame length:\",  \n",
    "       len(df_fdrop), \"\\nNumber of rows with at least 1 NA value: \", \n",
    "       (len(df_f)-len(df_fdrop))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fdrop.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_fdrop.default_next\n",
    "y.value_counts()\n",
    "\n",
    "x=df_fdrop.copy()\n",
    "x.drop(['default_next'],axis=1,inplace=True)\n",
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_logistic(x,y,features=[], do_scale=True,do_resamp=True,show_cm=False)\n",
    "predict_dtree(x,y,features=[], do_scale=True,do_resamp=True,show_cm=False)\n",
    "predict_rand_for(x,y,features=[], do_scale=True,do_resamp=True,show_cm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sub4=['limit_bal','pay_amt1','education', 'marriage','pay_0','pay_2',\n",
    "               'pay_amt1','pay_amt2','f5','f6','f7','f8','f10','f11'] \n",
    "predict_logistic(x,y,features=features_sub4, do_scale=True,do_resamp=True,show_cm=True)\n",
    "predict_dtree(x,y,features=features_sub4,do_scale=True,do_resamp=True,show_cm=True)\n",
    "predict_rand_for(x,y,features=features_sub4,do_scale=True,do_resamp=True,show_cm=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas for indicator variables and combined feature vairiables:\n",
    "1. Male and under 25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
