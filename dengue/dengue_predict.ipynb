{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Dengue Spread\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this notebook we examine feature imacting the occuance of Dengue Fever and employ several machine learning model to predict its spread. Dengue surveillance data is provided by the U.S. Centers for Disease Control and prevention, as well as the Department of Defense's Naval Medical Research Unit 6 and the Armed Forces Health Surveillance Center, in collaboration with the Peruvian government and U.S. universities. Environmental and climate data is provided by the National Oceanic and Atmospheric Administration (NOAA), an agency of the U.S. Department of Commerce.\n",
    "The project is part of the \"Predict the Next Pandemic Initiative\"\n",
    "\n",
    "The data set is hosted at Driven Data, which provides datasets that explore social and humanitarian issues. They also host machine learning competions, similar to Kaggle, but the overarching goal of all competitions is to provides some sort of humaniarial benefit. It's machine learning with a heart!\n",
    "\n",
    "Here is a link to Driven Data: https://www.drivendata.org/ \n",
    "More information on the Dengue issue can be found here: https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/page/81/\n",
    "\n",
    "If you are unfamiliar with Dengue Fever (it is uncommon in the US), is a mosquito-borne tropical disease common in more than 110 countries, mainly in Asia and South America. Each year between 50 and 528 million people are infected and approximately 10,000 to 20,000 die.\n",
    "The Wikipedia page is a good resouce: https://en.wikipedia.org/wiki/Dengue_fever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are the starting features\n",
    "\n",
    "City and date indicators\n",
    "* city – City abbreviations: sj for San Juan and iq for Iquitos\n",
    "* week_start_date – Date given in yyyy-mm-dd format\n",
    "\n",
    "NOAA's GHCN daily climate data weather station measurements\n",
    "* station_max_temp_c – Maximum temperature\n",
    "* station_min_temp_c – Minimum temperature\n",
    "* station_avg_temp_c – Average temperature\n",
    "* station_precip_mm – Total precipitation\n",
    "* station_diur_temp_rng_c – Diurnal temperature range\n",
    "\n",
    "PERSIANN satellite precipitation measurements (0.25x0.25 degree scale)\n",
    "* precipitation_amt_mm – Total precipitation\n",
    "\n",
    "NOAA's NCEP Climate Forecast System Reanalysis measurements (0.5x0.5 degree scale)\n",
    "* reanalysis_sat_precip_amt_mm – Total precipitation\n",
    "* reanalysis_dew_point_temp_k – Mean dew point temperature\n",
    "* reanalysis_air_temp_k – Mean air temperature\n",
    "* reanalysis_relative_humidity_percent – Mean relative humidity\n",
    "* reanalysis_specific_humidity_g_per_kg – Mean specific humidity\n",
    "* reanalysis_precip_amt_kg_per_m2 – Total precipitation\n",
    "* reanalysis_max_air_temp_k – Maximum air temperature\n",
    "* reanalysis_min_air_temp_k – Minimum air temperature\n",
    "* reanalysis_avg_temp_k – Average air temperature\n",
    "* reanalysis_tdtr_k – Diurnal temperature range\n",
    "\n",
    "Satellite vegetation - Normalized difference vegetation index (NDVI) - NOAA's CDR Normalized Difference Vegetation Index (0.5x0.5 degree scale) measurements\n",
    "* ndvi_se – Pixel southeast of city centroid\n",
    "* ndvi_sw – Pixel southwest of city centroid\n",
    "* ndvi_ne – Pixel northeast of city centroid\n",
    "* ndvi_nw – Pixel northwest of city centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable\n",
    "\n",
    "The target we are investigating is \"totol cases\", which is a specifies for each city, year, and weekofyear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets begin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = []\n",
    "for dirname, _, filenames in os.walk('.'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        data_path.append(os.path.join(dirname, filename))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_feat = pd.read_csv(data_path[1])\n",
    "df_targ = pd.read_csv(data_path[2])\n",
    "#print(df_targ.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_o will be our original dataframe, which will be left untouched\n",
    "df_o=pd.concat([df_feat,df_targ.total_cases],axis=1)\n",
    "df_o.sample(5)\n",
    "#df_o.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data are mostly floats, with a couple ints and a couple objects\n",
    "print('\\n Rows and Columns:')\n",
    "print(df_o.shape)\n",
    "print('\\n Data Types:')\n",
    "print(df_o.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function for displaying a correlation matrix for different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create working copy of the data for cleaning (df_o to remain unchanged)\n",
    "df = df_o.copy()\n",
    "df.isnull().sum().sort_values(ascending=False)\n",
    "#len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"ndvi_se\",\"ndvi_sw\",\"ndvi_ne\",\"ndvi_nw\"]].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could drop ndvi_ne as it is highly correlated with ndvi_nw, however currently we will try imputing with the mean of the other 3 ndvi variables. The 52 cases where ndvi_nw is missing, we will replace with the mean of ndvi_se and ndvi_sw. Where ndvi_se and ndvi_s are missing, we will impute with the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ndvi_se.fillna(df.ndvi_se.mean(),inplace=True)\n",
    "df.ndvi_sw.fillna(df.ndvi_sw.mean(),inplace=True)\n",
    "df.ndvi_nw.fillna((df.ndvi_sw + df.ndvi_se)/2,inplace=True)\n",
    "df.ndvi_ne.fillna((df.ndvi_sw + df.ndvi_se + df.ndvi_nw)/3,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will impute other rows with missing values with the mean of the respective column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will eventually going to drop year from the subesquent analysis, due to potential data leakage, but we will engineer some features using it later on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(\"year\", axis=1,inplace=True)\n",
    "#df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = df.loc[:,(df.dtypes=='object')]\n",
    "cat_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we will drop week_start_date. The day of the month likely has no bearing on dengue cases, week is already represented in its own feature/column, and year is being dropped due to leakage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('week_start_date', axis=1,inplace=True)\n",
    "#df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the city attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.city.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only two cities. Will do categorical encoding (though one-hot could also be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit([\"sj\",\"iq\"])\n",
    "#le.classes_\n",
    "df.city=le.transform(df.city)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Associations\n",
    "\n",
    "So, we no longer have any categorical variable, and no missing values. No lets take a look at correlation between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "def plot_corr_matrix(df, subset=[], details=\"All Features\"):\n",
    "\n",
    "    if len(subset)>0:\n",
    "        df = df[subset]\n",
    "        \n",
    "    corr = df.corr()\n",
    "    #cmap = sn.diverging_palette(255, 133, l=60, n=7, center=\"dark\")\n",
    "    \n",
    "    # The values number here ar just for \"nice\" scaling\n",
    "    xx=(df.shape[1]/3,(df.shape[1]/3)*.85 )\n",
    "    fig=plt.figure(figsize=xx)\n",
    "    ax = sn.heatmap(corr,cmap='RdBu_r',vmin=-1,vmax=1)\n",
    "\n",
    "    ax.set_title(\"Correlation for \"+details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by looking at all features; target variable is in the last row/column\n",
    "plot_corr_matrix(df=df, subset=[], details=\"All Features\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bottom row and right-most column on the above correlation matrix show how each feature correlates with total cases. We observe many features that are weekly-to-moderately correlated with total_case(0.2-0.4), but none that are ***strongly*** correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3.5))\n",
    "#%matplotlib inline\n",
    "df.corr().iloc[-1].sort_values(ascending=False).plot(kind=\"bar\")\n",
    "#plt.show()\n",
    "plt.xlabel('feature')\n",
    "plt.ylabel('Pearson Correaltion')\n",
    "plt.title('Feature Correlation (with Target)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation matrices for the ndvi features, before and after imputation\n",
    "plot_corr_matrix(df=df_o, subset=['ndvi_ne','ndvi_nw','ndvi_se','ndvi_se'], details=\"original\") \n",
    "plot_corr_matrix(df=df, subset=['ndvi_ne','ndvi_nw','ndvi_se','ndvi_se'], details=\"imputed\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Prepare the Data (split into x and y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.copy()\n",
    "x.drop(\"total_cases\", axis=1, inplace=True)\n",
    "y = df.total_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numberical featuers (all feature should now be numerical)\n",
    "num_features = x.loc[:,(df.dtypes=='int64')|(df.dtypes=='float64')|(df.dtypes=='int32')]\n",
    "num_features.columns\n",
    "num_features.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Linear Regression Model\n",
    "\n",
    "Now that we've done some basic data exploration and cleaning, let's do a preliminarly linear regression before any feature engineering. This will serve as starting point. We will just take numerical features--we will explore categorical encoding a bit later on. \n",
    "\n",
    "First let's funcationalize our linear regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg(x,y, features=[],do_scale=True, show_cm=False,normTF=True):\n",
    "    \n",
    "    if len(features)>0:\n",
    "        #print(len(features)>0)\n",
    "        x=x[features]\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x,y,test_size=0.2,random_state=3)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    if do_scale:\n",
    "        regressor = Pipeline(steps = [('scaler',StandardScaler()),('model',model)])\n",
    "    else:\n",
    "        regressor = Pipeline(steps = [('model',model)])\n",
    "        \n",
    "    regressor.fit(x_train,y_train)\n",
    "    y_predict = regressor.predict(x_val)\n",
    "    \n",
    "    scores = cross_val_score(regressor, x, y, cv=5, scoring='r2')\n",
    "    print(\"R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    # Display some random validation vs predicted y pairs\n",
    "    rand_samp = list(np.random.choice(range(len(y_val)),10,replace=False))\n",
    "    y_top = np.array(y_val.iloc[rand_samp])\n",
    "    # Rounding the predicted values for display purposes\n",
    "    yp_top = np.array(y_predict[rand_samp]).round()\n",
    "    \n",
    "    Y = np.array([y_top,yp_top]).T\n",
    "    print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def dtree_reg(x,y, features=[],do_scale=True, show_cm=False,normTF=True):\n",
    "    \n",
    "    if len(features)>0:\n",
    "        #print(len(features)>0)\n",
    "        x=x[features]\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x,y,test_size=0.2,random_state=3)\n",
    "    \n",
    "    model = DecisionTreeRegressor(random_state=3)\n",
    "    if do_scale:\n",
    "        regressor = Pipeline(steps = [('scaler',StandardScaler()),('model',model)])\n",
    "    else:\n",
    "        regressor = Pipeline(steps = [('model',model)])\n",
    "        \n",
    "    regressor.fit(x_train,y_train)\n",
    "    y_predict = regressor.predict(x_val)\n",
    "    \n",
    "    scores = cross_val_score(regressor, x, y, cv=5, scoring='r2')\n",
    "    print(\"R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    # Display some random validation vs predicted y pairs\n",
    "    rand_samp = list(np.random.choice(range(len(y_val)),10,replace=False))\n",
    "    y_top = np.array(y_val.iloc[rand_samp])\n",
    "    # Rounding the predicted values for display purposes\n",
    "    yp_top = np.array(y_predict[rand_samp]).round()\n",
    "    \n",
    "    Y = np.array([y_top,yp_top]).T\n",
    "    print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def randfor_reg(x,y, features=[],do_scale=True, show_cm=False,normTF=True):\n",
    "    \n",
    "    if len(features)>0:\n",
    "        #print(len(features)>0)\n",
    "        x=x[features]\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x,y,test_size=0.2,random_state=3)\n",
    "    \n",
    "    model = RandomForestRegressor(random_state=3)\n",
    "    if do_scale:\n",
    "        regressor = Pipeline(steps = [('scaler',StandardScaler()),('model',model)])\n",
    "    else:\n",
    "        regressor = Pipeline(steps = [('model',model)])\n",
    "        \n",
    "    regressor.fit(x_train,y_train)\n",
    "    y_predict = regressor.predict(x_val)\n",
    "    \n",
    "    scores = cross_val_score(regressor, x, y, cv=5, scoring='r2')\n",
    "    print(\"R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    # Display some random validation vs predicted y pairs\n",
    "    rand_samp = list(np.random.choice(range(len(y_val)),10,replace=False))\n",
    "    y_top = np.array(y_val.iloc[rand_samp])\n",
    "    # Rounding the predicted values for display purposes\n",
    "    yp_top = np.array(y_predict[rand_samp]).round()\n",
    "    \n",
    "    Y = np.array([y_top,yp_top]).T\n",
    "    print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Linear regression, no bell/whistles, to get a starting point\n",
    "# all numerical features\n",
    "lin_reg(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline scores for decision tree\n",
    "dtree_reg(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline scores for decision tree\n",
    "randfor_reg(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we're clearly starting from a place of poor prediction. Plenty of room for improvement!\n",
    "\n",
    "The plan going forward is to:\n",
    "* Perform categorical encoding for our categorical feature\n",
    "* Engineer some features\n",
    "* Explore some other regression models and choose 2-3 to fine-tune\n",
    "* Optimize our chosen models using regularization and gridsearc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "The first feature to engineer is an obvious one: number of case for the same city in the previous year. While we are at it, we could do it for a range of previous years. Lets limit it at 3.  The year range for this data is 1990-2010. So if we want to do a feature for each of the 3 previous year, problems arrise for 1990, 1991,and 1992. 1991 and 92 both have data avaible for **one** year previous, but 92 does not have data for **three** years previous, and and 91 does not for **two or three** years prior. So those years will have to be dealt with differently.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.sample(5)\n",
    "#df.set_index(['city','year','weekofyear'],inplace=True)\n",
    "#df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = (list(range(1993,2011)))\n",
    "df_sub=df.loc[(df.year<=2010)&(df.year>=1993)]\n",
    "print(years)\n",
    "#df_sub=df.loc[df.year.isin(years)]\n",
    "df_sub.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_sub)):\n",
    "    case_one_prior= df.loc[(df.city == df_sub.iloc[i].city)&(df.weekofyear == df_sub.iloc[i].weekofyear)&\n",
    "                           (df.year == (df_sub.iloc[i].year-1))].total_cases\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "5       False\n",
       "6       False\n",
       "7       False\n",
       "8       False\n",
       "9       False\n",
       "10      False\n",
       "11      False\n",
       "12      False\n",
       "13      False\n",
       "14      False\n",
       "15      False\n",
       "16      False\n",
       "17      False\n",
       "18      False\n",
       "19      False\n",
       "20      False\n",
       "21      False\n",
       "22      False\n",
       "23      False\n",
       "24      False\n",
       "25      False\n",
       "26      False\n",
       "27      False\n",
       "28      False\n",
       "29      False\n",
       "        ...  \n",
       "1426    False\n",
       "1427    False\n",
       "1428    False\n",
       "1429    False\n",
       "1430    False\n",
       "1431    False\n",
       "1432    False\n",
       "1433    False\n",
       "1434    False\n",
       "1435    False\n",
       "1436    False\n",
       "1437    False\n",
       "1438    False\n",
       "1439    False\n",
       "1440    False\n",
       "1441    False\n",
       "1442    False\n",
       "1443    False\n",
       "1444    False\n",
       "1445    False\n",
       "1446    False\n",
       "1447    False\n",
       "1448    False\n",
       "1449    False\n",
       "1450    False\n",
       "1451    False\n",
       "1452    False\n",
       "1453    False\n",
       "1454    False\n",
       "1455    False\n",
       "Length: 1456, dtype: bool"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.city == df_sub.iloc[i].city)&(df.weekofyear == df_sub.iloc[i].weekofyear)&(df.year == (df_sub.iloc[i].year-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.total_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "We will contiue to explore the linear regression model (lasso/ridge/elasticnet regularization explored later), but well will also investicate SVMs, Naive Bayes, Decision Trees. Ensemble methods will be exlored later on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
