{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_matrix(df, subset=[], details=\"All Features\"):\n",
    "    \"\"\"Plots a correlation matrix of all the features. Can specify subset of features if desired\"\"\"\n",
    "\n",
    "    if len(subset)>0:\n",
    "        df = df[subset]\n",
    "        \n",
    "    corr = df.corr()\n",
    "    #print(\"First 3 correlations: \"+str(corr.iloc[0,0:3]))\n",
    "    #cmap = sn.diverging_palette(255, 133, l=60, n=7, center=\"dark\")\n",
    "    \n",
    "    # The values number here ar just for \"nice\" scaling\n",
    "    xx=(df.shape[1]/3,(df.shape[1]/3)*.85 )\n",
    "    fig=plt.figure(figsize=xx)\n",
    "    ax = sn.heatmap(corr,cmap='RdBu_r',vmin=-1,vmax=1)\n",
    "\n",
    "    ax.set_title(\"Correlation for \"+details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_target_to_end(df,target_label):\n",
    "    \n",
    "    \"\"\"Moves the target colums to the end of the DataFrame for easier book-keeping\n",
    "    and correlation matrix display\"\"\"\n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    \n",
    "    p = cols.index(target_label)\n",
    "    cols_new=cols[0:p]\n",
    "    cols_new = cols_new + cols[p+1:]\n",
    "    cols_new.append(cols[p])\n",
    "    \n",
    "    df=df[cols_new]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_features(df,target_label,n_features=10,exclude_labels=None):\n",
    "    \"\"\" Select the n features most highly correlated with the specified \n",
    "    target aviables. NOTE: should only run this with df that has the target \n",
    "    still embedded; will return erroneous result if used e.g. with x\"\"\"\n",
    "    \n",
    "    # Set upper limit on number of features that can be returned\n",
    "    if exclude_labels is not None:\n",
    "        length_constraint = len(df.columns)-1-len(exclude_labels)\n",
    "    else:\n",
    "        length_constraint = len(df.columns)-1\n",
    "    \n",
    "    # if n_features is too high, cap it and print a warning\n",
    "    if n_features>length_constraint:\n",
    "        print(\"warning: number of features cannot exceed \"+str(length_constraint)+\n",
    "              \"\\nSelecting \"+str(length_constraint)+\"features.\")\n",
    "        n_features = length_constraint\n",
    "    \n",
    "    df=move_target_to_end(df,target_label)\n",
    "    \n",
    "    corr_vals_sorted=abs(df.corr()).iloc[-1].sort_values(ascending=False)\n",
    "    #print(exclude_labels)\n",
    "    high_corr_feat = list(corr_vals_sorted.index[1:n_features+1])\n",
    "    \n",
    "    # count the excluded labels\n",
    "    c=0\n",
    "    if exclude_labels is not None:\n",
    "        for li in range(len(exclude_labels)):\n",
    "            if exclude_labels[li] in high_corr_feat:\n",
    "                high_corr_feat.remove(exclude_labels[li]) \n",
    "                c+=1\n",
    "                high_corr_feat.append(corr_vals_sorted.index[n_features+c])\n",
    "            \n",
    "    #print(high_corr_feat)\n",
    "    return high_corr_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr_feat_ind(df,corr_max,target_label, ignore_feats):\n",
    "    \n",
    "    # ensure target label is at the very end\n",
    "    move_target_to_end(df,target_label)\n",
    "    df = df.copy()\n",
    "    \n",
    "    corr_all = df.corr()\n",
    "    corr_pass_tf = (abs(corr_all)>corr_max)&(abs(corr_all<1))\n",
    "    #print(corr_pass_tf)\n",
    "\n",
    "    ans = np.where(corr_pass_tf)\n",
    "    row = ans[0]\n",
    "    col = ans[1]    \n",
    "    unique_pairs = []\n",
    "    \n",
    "    # add row/column pairs corresponding to correlated features\n",
    "    for i in range(len(row)):\n",
    "        pairs=(row[i],col[i])\n",
    "        unique_pairs.append(pairs)\n",
    "\n",
    "    # Remove conjugate/reciprical pairs\n",
    "    i=0\n",
    "    while i in range(len(unique_pairs)):\n",
    "        tup_i = unique_pairs[i]\n",
    "        # flip the order of the two numbers\n",
    "        tup_i_rev = (tup_i[1],tup_i[0])\n",
    "        unique_pairs.remove(tup_i_rev)\n",
    "        i+=1\n",
    "        \n",
    "    # exclude and pairs that include the target variable or exlude list\n",
    "    \n",
    "    # t_ind is index for target feature (last/row col)      \n",
    "    t_ind = [len(corr_all)-1]\n",
    "    \n",
    "    # find indices of ignore features\n",
    "    col_list = list(df.columns)\n",
    "    ignore_ind = []\n",
    "    for f in range(len(ignore_feats)):\n",
    "        I = col_list.index(ignore_feats[f])\n",
    "        ignore_ind.append(I)\n",
    "    \n",
    "    z = t_ind + ignore_ind        \n",
    "    print(\"Indices of Target and Ignored features: \"+str(z))\n",
    "    \n",
    "    # unique pairs copy (upc)\n",
    "    upc= unique_pairs.copy()\n",
    "    R1 = range(len(unique_pairs))\n",
    "    for i in R1: \n",
    "        print(\"unique pairs:\" +str(unique_pairs[i]))\n",
    "        \n",
    "        # z is list of indices for target and ignored features       \n",
    "        # exclude condition\n",
    "        ex_cond = [up_i in z for up_i in unique_pairs[i]]\n",
    "        if any(ex_cond):\n",
    "            tup_i = unique_pairs[i]\n",
    "            upc.remove(tup_i)\n",
    "    unique_pairs = upc\n",
    "    return unique_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr_remove_list(df,target_label,feature_pairs):\n",
    "    # Remove one feature from each pair. which ever is LEAST correlated with the target var\n",
    "    features_remove_list = []\n",
    "    target_label='total_cases'\n",
    "    for i in range(len(feature_pairs)):\n",
    "        print(str(i)+\" - \"+df.columns[feature_pairs[i][0]]+\" vs. \"+df.columns[feature_pairs[i][1]])\n",
    "        # take the absolute val of the corr between target and first feature [0 index] in the pair\n",
    "        corr0 = abs(df[[target_label,df.columns[feature_pairs[i][0]]]].corr().min()[0])\n",
    "        print(\"corr of 1st feature w target: \"+str(corr0))\n",
    "\n",
    "        # take the absolute val of the corr between target and first feature [0 index] in the pair\n",
    "        corr1 = abs(df[[target_label,df.columns[feature_pairs[i][1]]]].corr().min()[0])\n",
    "        print(\"corr of 2st feature w target: \"+str(corr1))\n",
    "\n",
    "        corrs = [corr0,corr1]\n",
    "        min_i = corrs.index(min(corrs))\n",
    "        features_remove_list.append(df.columns[feature_pairs[i][min_i]])\n",
    "        print(\"Exclude feature \"+str(min_i+1)+\": \"+features_remove_list[i]+\"\\n\")\n",
    "        #print(features_remove_list)\n",
    "    return features_remove_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg(x,y, features=[],do_scale=True, show_cm=False,normTF=True):\n",
    "    \n",
    "    if len(features)>0:\n",
    "        #print(len(features)>0)\n",
    "        x=x[features]\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x,y,test_size=0.2,random_state=3)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    if do_scale:\n",
    "        regressor = Pipeline(steps = [('scaler',StandardScaler()),('model',model)])\n",
    "    else:\n",
    "        regressor = Pipeline(steps = [('model',model)])\n",
    "        \n",
    "    regressor.fit(x_train,y_train)\n",
    "    y_predict = regressor.predict(x_val)\n",
    "    \n",
    "    scores = cross_val_score(regressor, x, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    print(\"RMSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    # Display some random validation vs predicted y pairs\n",
    "    rand_samp = list(np.random.choice(range(len(y_val)),10,replace=False))\n",
    "    y_top = np.array(y_val.iloc[rand_samp])\n",
    "    # Rounding the predicted values for display purposes\n",
    "    yp_top = np.array(y_predict[rand_samp]).round()\n",
    "    \n",
    "    Y = np.array([y_top,yp_top]).T\n",
    "    print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def dtree_reg(x,y, features=[],do_scale=True, show_cm=False,normTF=True):\n",
    "    \n",
    "    if len(features)>0:\n",
    "        #print(len(features)>0)\n",
    "        x=x[features]\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x,y,test_size=0.2,random_state=3)\n",
    "    \n",
    "    model = DecisionTreeRegressor(random_state=3)\n",
    "    if do_scale:\n",
    "        regressor = Pipeline(steps = [('scaler',StandardScaler()),('model',model)])\n",
    "    else:\n",
    "        regressor = Pipeline(steps = [('model',model)])\n",
    "        \n",
    "    regressor.fit(x_train,y_train)\n",
    "    y_predict = regressor.predict(x_val)\n",
    "    \n",
    "    scores = cross_val_score(regressor, x, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    print(\"RMSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    # Display some random validation vs predicted y pairs\n",
    "    rand_samp = list(np.random.choice(range(len(y_val)),10,replace=False))\n",
    "    y_top = np.array(y_val.iloc[rand_samp])\n",
    "    # Rounding the predicted values for display purposes\n",
    "    yp_top = np.array(y_predict[rand_samp]).round()\n",
    "    \n",
    "    Y = np.array([y_top,yp_top]).T\n",
    "    print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def randfor_reg(x,y, features=[],do_scale=True, show_cm=False,normTF=True):\n",
    "    \n",
    "    if len(features)>0:\n",
    "        #print(len(features)>0)\n",
    "        x=x[features]\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x,y,test_size=0.2,random_state=3)\n",
    "    \n",
    "    model = RandomForestRegressor(random_state=3, n_estimators=100)\n",
    "    if do_scale:\n",
    "        regressor = Pipeline(steps = [('scaler',StandardScaler()),('model',model)])\n",
    "    else:\n",
    "        regressor = Pipeline(steps = [('model',model)])\n",
    "        \n",
    "    regressor.fit(x_train,y_train)\n",
    "    y_predict = regressor.predict(x_val)\n",
    "    \n",
    "    scores = cross_val_score(regressor, x, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    print(\"RMSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    # Display some random validation vs predicted y pairs\n",
    "    rand_samp = list(np.random.choice(range(len(y_val)),10,replace=False))\n",
    "    y_top = np.array(y_val.iloc[rand_samp])\n",
    "    # Rounding the predicted values for display purposes\n",
    "    yp_top = np.array(y_predict[rand_samp]).round()\n",
    "    \n",
    "    Y = np.array([y_top,yp_top]).T\n",
    "    print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def print_gridsearch_results(clf,x_valid, y_valid):\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    #y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    y_pred = clf.predict(x_valid)\n",
    "    print(classification_report(y_valid, y_pred))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
